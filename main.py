import pandas as pd  # used for DATAFrames and DataFrames can hold different types data of multidimensional arrays.
import  numpy as np  # Numpy provides robust data structures for efficient computation of multi-dimensional arrays & matrices.
import pickle
import sklearn.ensemble as ske
from sklearn import tree, linear_model
from sklearn.model_selection import train_test_split

from sklearn.feature_selection import SelectFromModel
from sklearn.naive_bayes import GaussianNB
import joblib
from sklearn.metrics import confusion_matrix

data = pd.read_csv('data.csv', sep='|')  # generate df as data
x = data.drop(['Name', 'md5', 'legitimate'],
              axis=1).values  # now droping some coloumns as axis 1(mean coloumn) and will show the values in the rows
y = data['legitimate'].values  # values of legitimate data

print('%i total features\n' % x.shape[1])  # shape() is use in pandas to give number.

feature_sel = ske.ExtraTreesClassifier().fit(x, y)  # Build a forest of trees from the training set (x, y).
model = SelectFromModel(feature_sel, prefit=True)  # for selecting features based on importance weights.
x_new = model.transform(x)  # Reduce X to the selected features.
nb_features = x_new.shape[1]  # save value 13 as shape is (130847, 13)
y_encoded = np.zeros(y.shape, dtype=np.int8)

x_train, x_test, y_train, y_test = train_test_split(x_new, y,test_size=0.2)  # now converting in training and testing data in 20% range hahhahaha ! as total x is 130847 and testing is 130847*0.2=27610 :)
features = []

print('Important features %i' % nb_features)  # as mentioned above

indices = np.argsort(feature_sel.feature_importances_)[::-1][:nb_features]
for f in range(nb_features):
    print("%d. Feature %s (%f)" % (f + 1, data.columns[2 + indices[f]], feature_sel.feature_importances_[indices[f]]))

for f in sorted(np.argsort(feature_sel.feature_importances_)[::-1][:nb_features]):  # [::-1] mean start with last towards first
    features.append(data.columns[2 + f])

algorithms = {
    "DecisionTree": tree.DecisionTreeClassifier(max_depth=10),
    # The max_depth parameter denotes maximum depth of the tree.

    "RandomForest": ske.RandomForestClassifier(n_estimators=50),
    # In case, of random forest, these ensemble classifiers are the randomly created decision trees. Each decision tree is a single classifier and the target prediction is based on the majority voting method.
    # n_estimators ==The number of trees in the forest.

    "GradientBoosting": ske.GradientBoostingClassifier(n_estimators=50),
    "AdaBoost": ske.AdaBoostClassifier(n_estimators=100),
    # Ada mean Adaptive
    # Both of the algorithms are boosting algorithms which means that they convert a set of weak learners into a single strong learner. They both initialize a strong learner (usually a decision tree) and iteratively create a weak learner that is added to the strong learner.

    "GNB": GaussianNB()
    # Bayes theorem is based on conditional probability. The conditional probability helps us calculating the probability that something will happen
}

results = {}
print("\n Testing algorithms")
for i in algorithms:
    win = algorithms[i]
    win.fit(x_train, y_train)  # fit may be called as 'trained'
    score = win.score(x_test, y_test)
    print("%s : %f %%" % (i, score * 100))
    results[i] = score

winner = max(results, key=results.get)
print('\nWinner algorithm is %s with a %f %% success' % (winner, results[winner] * 100))

print('Saving algorithm and feature list in classifier directory...')
joblib.dump(algorithms[winner], 'classifier/classifier.pkl')  # Persist an arbitrary Python object into one file.
open('classifier/features.pkl', 'wb').write(pickle.dumps(features))
# joblib works especially well with NumPy arrays which are used by sklearn so depending on the classifier type you use you might have performance and size benefits using joblib.Otherwise pickle does work correctly so saving a trained classifier and loading it again will produce the same results no matter which of the serialization libraries you use
print('Saved')

win = algorithms[winner]
res = win.predict(x_test)
mt = confusion_matrix(y_test, res)
# A confusion matrix, also known as an error matrix,[4] is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning.
print("False positive rate : %f %%" % ((mt[0][1] / float(sum(mt[0]))) * 100))
print('False negative rate : %f %%' % ((mt[1][0] / float(sum(mt[1])) * 100)))
print('True positive rate : %f %%' % ((mt[1][1] / float(sum(mt[1])) * 100)))
print('True negative rate : %f %%' % ((mt[0][0] / float(sum(mt[0])) * 100)))


